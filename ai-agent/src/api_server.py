"""FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è AI –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π trace output."""

import os
import asyncio
from typing import Optional, List, Dict, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv, find_dotenv
from structured_logging import get_logger

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(find_dotenv())

import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from a2a_wrapper import create_mcp_tools
from agent import AgentWithModelSwitch, ModelManager

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è structured logger
logger = get_logger("ai-agent")

app = FastAPI(title="GitHub AI Analyst API", version="1.0.0")

# CORS middleware –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã –≤–º–µ—Å—Ç–æ "*" –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:8501",
        "http://frontend:8501",
        "http://127.0.0.1:8501",
        # –î–æ–±–∞–≤–∏—Ç—å production –¥–æ–º–µ–Ω—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–≥–µ–Ω—Ç–∞
agent_wrapper: Optional[AgentWithModelSwitch] = None
trace_log: List[Dict[str, Any]] = []


class QueryRequest(BaseModel):
    """–ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è API."""
    query: str
    model_alias: Optional[str] = None


class TraceEntry(BaseModel):
    """–ú–æ–¥–µ–ª—å –∑–∞–ø–∏—Å–∏ trace."""
    type: str  # "thought", "tool_call", "observation", "error"
    content: str
    status: Optional[str] = None  # "success", "error", "pending"
    tool_name: Optional[str] = None


class QueryResponse(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ API."""
    response: str
    trace: List[TraceEntry]
    model_used: str


def log_trace(entry_type: str, content: str, status: Optional[str] = None, tool_name: Optional[str] = None):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø–∏—Å—å –≤ trace –ª–æ–≥."""
    trace_log.append({
        "type": entry_type,
        "content": content,
        "status": status,
        "tool_name": tool_name
    })


def clear_trace():
    """–û—á–∏—â–∞–µ—Ç trace –ª–æ–≥."""
    trace_log.clear()


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞."""
    global agent_wrapper
    try:
        tools = create_mcp_tools()
        agent_wrapper = AgentWithModelSwitch(tools)
        logger.info(
            "AI Agent initialized successfully",
            tools_count=len(tools),
            current_model=agent_wrapper.get_current_model()
        )
    except Exception as e:
        logger.error(
            "Failed to initialize AI Agent",
            error=str(e),
            error_type=type(e).__name__
        )
        raise


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint."""
    return {
        "service": "GitHub AI Analyst API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """
    Healthcheck endpoint –¥–ª—è Docker –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
    
    Returns:
        dict: –°—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {
        "status": "healthy",
        "agent_initialized": agent_wrapper is not None,
        "service": "GitHub AI Analyst API",
        "version": "1.0.0"
    }


@app.get("/api/models")
async def get_models():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    manager = ModelManager()
    aliases = manager.get_available_aliases()
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –∏–∫–æ–Ω–∫–∞–º–∏
    model_list = []
    icon_map = {
        "GigaChat": "üìß",
        "Sage": "üß†",
        "MiniMax": "‚ö°",
        "GLM": "üìç",
        "GPT-OSS": "ü§ñ",
        "Qwen-Coder": "üíª",
        "Qwen-Large": "üìä",
        "Qwen-Next": "üöÄ",
        "T-Lite-1.0": "‚öôÔ∏è",
        "T-Pro-1.0": "üë§",
        "T-Pro-2.0": "üì∑"
    }
    
    primary_aliases = [
        "GigaChat", "Sage", "MiniMax", "GLM", "GPT-OSS",
        "Qwen-Coder", "Qwen-Large", "Qwen-Next",
        "T-Lite-1.0", "T-Pro-1.0", "T-Pro-2.0"
    ]
    
    for alias in primary_aliases:
        if alias in manager.MODEL_ALIASES:
            model_list.append({
                "alias": alias,
                "model": manager.MODEL_ALIASES[alias],
                "icon": icon_map.get(alias, "üìÑ")
            })
    
    return {
        "models": model_list,
        "current_model": agent_wrapper.get_current_model() if agent_wrapper else None
    }


@app.post("/api/query", response_model=QueryResponse)
async def process_query(request: QueryRequest):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ AI Agent.
    
    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∞–ª–∏–∞—Å–æ–º –º–æ–¥–µ–ª–∏
        
    Returns:
        –û—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞ —Å trace –ª–æ–≥–æ–º
    """
    global agent_wrapper
    
    logger.info(
        "Received query request",
        query_length=len(request.query),
        model_alias=request.model_alias
    )
    
    if agent_wrapper is None:
        logger.error("AI Agent not initialized when processing query")
        raise HTTPException(status_code=500, detail="AI Agent not initialized")
    
    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π trace
    clear_trace()
    
    # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∞–ª–∏–∞—Å
    if request.model_alias:
        success, message = agent_wrapper.switch_model(request.model_alias)
        if success:
            logger.info(f"Model switched successfully: {message}", model_alias=request.model_alias)
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∞–∫—Ç–∏–≤–Ω–∞ –∏–ª–∏ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, –Ω–µ –ª–æ–≥–∏—Ä—É–µ–º
    
    current_model = agent_wrapper.get_current_model()
    log_trace("thought", f"Processing query: {request.query}")
    
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞
        log_trace("thought", f"Analyzing query: {request.query}")
        log_trace("thought", "Selecting appropriate tools for the query...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        try:
            result = await agent_wrapper.agent.ainvoke({
                "input": request.query
            })
        except Exception as agent_error:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Ö–µ–º—ã
            error_str = str(agent_error)
            if "422" in error_str or "Type properties" in error_str or "args.items.type" in error_str:
                log_trace("error", f"Schema validation error: {error_str}", "error")
                # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(
                    api_key=os.getenv("API_KEY"),
                    base_url=os.getenv("BASE_URL", "https://foundation-models.api.cloud.ru/v1"),
                    model=current_model,
                    temperature=0.5
                )
                simple_response = await llm.ainvoke(request.query)
                response_text = simple_response.content if hasattr(simple_response, 'content') else str(simple_response)
                log_trace("observation", "Query processed with direct LLM response (tool schema error)", "success")
                
                trace_entries = [
                    TraceEntry(
                        type=entry["type"],
                        content=entry["content"],
                        status=entry.get("status"),
                        tool_name=entry.get("tool_name")
                    )
                    for entry in trace_log
                ]
                
                return QueryResponse(
                    response=response_text,
                    trace=trace_entries,
                    model_used=current_model
                )
            else:
                raise
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
        response_text = result.get("output", "No response generated")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è trace
        intermediate_steps = result.get("intermediate_steps", [])
        for step in intermediate_steps:
            if len(step) >= 2:
                # step[0] - —ç—Ç–æ AgentAction, step[1] - —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                action = step[0]
                tool_result = step[1]
                
                tool_name = action.tool if hasattr(action, 'tool') else str(action)
                tool_input = action.tool_input if hasattr(action, 'tool_input') else {}
                
                log_trace("tool_call", f"Calling tool: {tool_name} with input: {tool_input}", "success", tool_name)
                log_trace("observation", f"Tool result: {str(tool_result)[:300]}...", "success")
        
        log_trace("observation", "Query processed successfully", "success")
        
        logger.info(
            "Query processed successfully",
            model=current_model,
            response_length=len(response_text),
            trace_entries_count=len(trace_log)
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º trace entries
        trace_entries = [
            TraceEntry(
                type=entry["type"],
                content=entry["content"],
                status=entry.get("status"),
                tool_name=entry.get("tool_name")
            )
            for entry in trace_log
        ]
        
        return QueryResponse(
            response=response_text,
            trace=trace_entries,
            model_used=current_model
        )
        
    except Exception as e:
        error_msg = f"Error processing query: {str(e)}"
        logger.error(
            "Error processing query",
            error=str(e),
            error_type=type(e).__name__,
            model=current_model
        )
        log_trace("error", error_msg, "error")
        
        trace_entries = [
            TraceEntry(
                type=entry["type"],
                content=entry["content"],
                status=entry.get("status"),
                tool_name=entry.get("tool_name")
            )
            for entry in trace_log
        ]
        
        raise HTTPException(
            status_code=500,
            detail={
                "error": error_msg,
                "trace": [e.dict() for e in trace_entries]
            }
        )


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=port,
        reload=False
    )

